{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x22420f78350>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [item if item in self.str_to_int            #1\n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)    #2\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 69\n"
     ]
    }
   ],
   "source": [
    "# unicode code point for each character\n",
    "\n",
    "text = \"\"\"Want to buy the textbooks from the bookshop, The bookshop is in Cairo\"\"\"\n",
    "\n",
    "encodings = list(text.encode(\"utf-8\"))\n",
    "print(len(encodings), len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87, 97, 110, 116, 32, 116, 111, 32, 98, 117, 121, 32, 116, 104, 101, 32, 116, 101, 120, 116, 98, 111, 111, 107, 115, 32, 102, 114, 111, 109, 32, 116, 104, 101, 32, 98, 111, 111, 107, 115, 104, 111, 112, 44, 32, 84, 104, 101, 32, 98, 111, 111, 107, 115, 104, 111, 112, 32, 105, 115, 32, 105, 110, 32, 67, 97, 105, 114, 111]\n"
     ]
    }
   ],
   "source": [
    "print(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_freq_text(text):\n",
    "    encodings = list(text.encode(\"utf-8\"))\n",
    "    return encodings\n",
    "\n",
    "\n",
    "def get_pairs(encodings):\n",
    "    return zip(encodings, encodings[1:])\n",
    "\n",
    "def get_freq_ids(encodings):\n",
    "    pairs = get_pairs(encodings)\n",
    "    pair_counts = Counter(pairs)\n",
    "    \n",
    "    pair_dict = {pair: count for pair, count in sorted(pair_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return pair_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_freq(pair_freq):\n",
    "\n",
    "    #rank the keys by value\n",
    "    return max(pair_freq, key=pair_freq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87, 97, 110, 116, 32, 116, 111, 32, 98, 117, 121, 32, 116, 104, 101, 32, 116, 101, 120, 116, 98, 111, 111, 107, 115, 32, 102, 114, 111, 109, 32, 116, 104, 101, 32, 98, 111, 111, 107, 115, 104, 111, 112, 44, 32, 84, 104, 101, 32, 98, 111, 111, 107, 115, 104, 111, 112, 32, 105, 115, 32, 105, 110, 32, 67, 97, 105, 114, 111]\n"
     ]
    }
   ],
   "source": [
    "print(get_freq_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bytes_ids = get_freq_text(text)\n",
    "\n",
    "max_vocab_id = max(set(text_bytes_ids))\n",
    "vocab = sorted(set(text_bytes_ids))\n",
    "vocab_str = [chr(vocab[i]) for i in range(0, len(vocab))]\n",
    "\n",
    "vocab_dict = dict(zip(vocab, vocab_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of bytes in text = 69, the number of vocab = 22\n"
     ]
    }
   ],
   "source": [
    "print(\"the number of bytes in text = {}, the number of vocab = {}\".format(len(text_bytes_ids), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_bytes_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_bytes_ids[67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ids(text_bytes_ids, vocab_dict, most_freq):\n",
    "    \n",
    "    # merge the most frequent pair\n",
    "    max_vocab_id = max(set(text_bytes_ids))\n",
    "\n",
    "    for i in range(len(text_bytes_ids) - 1):\n",
    "        if (text_bytes_ids[i], text_bytes_ids[i + 1]) == most_freq:\n",
    "            text_bytes_ids[i] = max_vocab_id + 1\n",
    "            text_bytes_ids[i + 1] = None\n",
    "    \n",
    "    decoded_new_vocab = ''.join(vocab_dict.get(i) for i in most_freq)\n",
    "    \n",
    "    vocab_dict[max_vocab_id + 1] = decoded_new_vocab\n",
    "    \n",
    "    text_bytes_ids = [id for id in text_bytes_ids if id is not None]\n",
    "    \n",
    "    return text_bytes_ids, vocab_dict, max_vocab_id + 1\n",
    "\n",
    "def merge_ids_encoding(text_bytes_ids, pair, position):\n",
    "\n",
    "    for i in range(len(text_bytes_ids) - 1):\n",
    "        if (text_bytes_ids[i], text_bytes_ids[i + 1]) == pair:\n",
    "            text_bytes_ids[i] = position\n",
    "            text_bytes_ids[i + 1] = None\n",
    "        \n",
    "    text_bytes_ids = [id for id in text_bytes_ids if id is not None]\n",
    "    \n",
    "    return text_bytes_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(text_bytes_ids, vocab_dict, num_merges):\n",
    "\n",
    "    merged_dict = {}\n",
    "    for i in range(num_merges):\n",
    "        pair_freq = get_freq_ids(text_bytes_ids)\n",
    "        most_freq = get_most_freq(pair_freq)\n",
    "        print(most_freq)\n",
    "        text_bytes_ids, vocab_dict, merge_id = merge_ids(text_bytes_ids, vocab_dict, most_freq)\n",
    "        merged_dict[most_freq] = merge_id\n",
    "\n",
    "    return text_bytes_ids, vocab_dict, merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 116)\n",
      "(32, 98)\n",
      "(104, 101)\n",
      "(111, 111)\n",
      "(125, 107)\n",
      "(126, 115)\n",
      "(122, 124)\n",
      "(114, 111)\n",
      "(123, 127)\n",
      "(130, 104)\n",
      "(131, 111)\n",
      "(132, 112)\n",
      "(32, 105)\n",
      "(87, 97)\n",
      "(135, 110)\n",
      "(136, 116)\n"
     ]
    }
   ],
   "source": [
    "text_bytes_ids, vocab_dict, merged_dict = merge_tokens(text_bytes_ids, vocab_dict, num_merges = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the string bytes length decreased by 4 which is the frequency of the most frequent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ids(text_ids, vocab_dict):\n",
    "    decoded_text = ''.join(vocab_dict.get(i) for i in text_ids)\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Want to buy the textbooks from the bookshop, The bookshop is in Cairo'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_ids(text_bytes_ids, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_str):\n",
    "    token_ids = list(text_str.encode(\"utf-8\"))\n",
    "\n",
    "    while len(token_ids) >= 2:\n",
    "\n",
    "        pairs = list(get_pairs(token_ids))\n",
    "        min_merge_pair = min(pairs, key= lambda p: merged_dict.get(p, float('inf')))\n",
    "        \n",
    "        if min_merge_pair not in merged_dict:\n",
    "            break\n",
    "        else:\n",
    "            token_ids = merge_ids_encoding(token_ids, min_merge_pair, merged_dict[min_merge_pair])\n",
    "        \n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"book in a bookshop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 7)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(test.encode(\"utf-8\"))), len(encode(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book in a bookshop'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_ids(encode(test), vocab_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
